{
 "metadata": {
  "name": "",
  "signature": "sha256:b18ff9f9e66826d2ef5a60771e55aa51578940a3487d2ecab8804c5078d0c73b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import sys\n",
      "import pandas as pd\n",
      "import math\n",
      "\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "# turn of data table rendering\n",
      "pd.set_option('display.notebook_repr_html', False)\n",
      "sys.version"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "'2.7.8 |Anaconda 2.1.0 (64-bit)| (default, Jul  2 2014, 15:12:11) [MSC v.1500 64 bit (AMD64)]'"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv('data/entropy.csv')\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "  gender personality work  happiness\n",
        "0    man        rude   no  not happy\n",
        "1    man        nice   no  not happy\n",
        "2  woman        rude  yes      happy\n",
        "3    man        nice  yes      happy"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Entropy\n",
      "We first have to choose the starting node of our decision tree. We do this byu determining the information gain of each feature. The more information we gain by splitting the data on a certain feature the better. [Shannon's Entropy Formula](http://crackingthenutshell.com/what-is-information-part-2a-information-theory/) where entropy is denoted as $H$:\n",
      "\n",
      "$$\n",
      "H=-\\sum_{i}(P_i)\\cdot log_2(P_i)\n",
      "$$\n",
      "\n",
      "The sum over all the data points of a label"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate the happiness label entropy\n",
      "n = 4.0\n",
      "p_happy = 2.0 / n  # probability of happy in the data set\n",
      "p_nothappy = 1.0 - p_happy\n",
      "entropy_happiness = -p_happy * math.log(p_happy, 2) - p_nothappy * math.log(p_nothappy, 2)\n",
      "entropy_happiness"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "1.0"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Information Gain\n",
      "We use Shannon's information Gain to decide which feature to use to split the data. The goal is the choose the feature that maximizes the information gain.\n",
      "\n",
      "$$\n",
      "information\\ gain=entropy(parent)- \\begin{bmatrix}weighted\\\\average\\end{bmatrix}entropy(children)\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's try to split happiness based on gender\n",
      "n_man = 3.0\n",
      "n_woman = 1.0\n",
      "p_man_happy = 2.0 / n_man\n",
      "p_man_nothappy = 1.0 / n_man\n",
      "\n",
      "entropy_man_happiness = -p_man_happy * math.log(p_man_happy, 2) \\\n",
      "                        -p_man_nothappy * math.log(p_man_nothappy, 2)\n",
      "\n",
      "p_woman_happy = 1.0 / n_woman\n",
      "p_woman_nothappy = 0.0 / n_woman\n",
      "\n",
      "entropy_woman_happiness = -p_woman_happy * math.log(p_woman_happy, 2) \\\n",
      "                          -p_woman_nothappy * 0\n",
      "\n",
      "\n",
      "print('Entropy man node:', entropy_man_happiness)\n",
      "print('Entropy woman node:', entropy_woman_happiness)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Entropy man node: 0.918295834054\n",
        "Entropy woman node: 0.0\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Calculate the weighted average of the entropy of the children\n",
      "n_happiness_split_by_man = 3.0 / 4.0\n",
      "n_happiness_split_by_woman = 1 - n_happiness_split_by_man\n",
      "\n",
      "entropy_children = (n_happiness_split_by_man * entropy_man_happiness - \\\n",
      "                    n_happiness_split_by_woman * entropy_woman_happiness)\n",
      "entropy_children"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "0.6887218755408672"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# information gain if we split on gender now:\n",
      "entropy_happiness - entropy_children"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0.31127812445913283"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}